{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /tmp/openai-2018-09-10-18-48-46-274326\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from baselines import deepq\n",
    "\n",
    "def callback(lcl, _glb):\n",
    "    # stop training if reward exceeds 199\n",
    "    is_solved = lcl['t'] > 100 and sum(lcl['episode_rewards'][-101:-1]) / 100 >= 199\n",
    "    return is_solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cartpole():\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    act = deepq.learn(\n",
    "        env,\n",
    "        network='mlp',\n",
    "        lr=1e-3,\n",
    "        total_timesteps=10000,\n",
    "        buffer_size=50000,\n",
    "        exploration_fraction=0.1,\n",
    "        exploration_final_eps=0.02,\n",
    "        print_freq=10,\n",
    "        callback=callback\n",
    "    )\n",
    "    print(\"Saving model to cartpole_model.pkl\")\n",
    "    #act.save(\"cartpole_model.pkl\")\n",
    "    return act\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cartpole(act):\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "#     act = deepq.load(\"cartpole_model.pkl\")\n",
    "\n",
    "    while True:\n",
    "        obs, done = env.reset(), False\n",
    "        episode_rew = 0\n",
    "        while not done:\n",
    "#             env.render()\n",
    "            obs, rew, done, _ = env.step(act(obs[None])[0])\n",
    "            episode_rew += rew\n",
    "        print(\"Episode reward\", episode_rew)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mountaincar():\n",
    "    env = gym.make(\"MountainCar-v0\")\n",
    "    # Enabling layer_norm here is import for parameter space noise!\n",
    "    model = deepq.models.mlp([64], layer_norm=True)\n",
    "    act = deepq.learn(\n",
    "        env,\n",
    "        q_func=model,\n",
    "        lr=1e-3,\n",
    "        max_timesteps=10000,\n",
    "        buffer_size=50000,\n",
    "        exploration_fraction=0.1,\n",
    "        exploration_final_eps=0.1,\n",
    "        print_freq=10,\n",
    "        param_noise=True\n",
    "    )\n",
    "    print(\"Saving model to mountaincar_model.pkl\")\n",
    "    act.save(\"mountaincar_model.pkl\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mountaincar():\n",
    "    env = gym.make(\"MountainCar-v0\")\n",
    "    act = deepq.load(\"mountaincar_model.pkl\")\n",
    "\n",
    "    while True:\n",
    "        obs, done = env.reset(), False\n",
    "        episode_rew = 0\n",
    "        while not done:\n",
    "            env.render()\n",
    "            obs, rew, done, _ = env.step(act(obs[None])[0])\n",
    "            episode_rew += rew\n",
    "    print(\"Episode reward\", episode_rew)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 83       |\n",
      "| episodes                | 10       |\n",
      "| mean 100 episode reward | 18.6     |\n",
      "| steps                   | 166      |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deeprl/anaconda3/envs/deeprl2/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/deeprl/anaconda3/envs/deeprl2/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 66       |\n",
      "| episodes                | 20       |\n",
      "| mean 100 episode reward | 17.8     |\n",
      "| steps                   | 338      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 52       |\n",
      "| episodes                | 30       |\n",
      "| mean 100 episode reward | 16.8     |\n",
      "| steps                   | 486      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 34       |\n",
      "| episodes                | 40       |\n",
      "| mean 100 episode reward | 17.3     |\n",
      "| steps                   | 672      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 22       |\n",
      "| episodes                | 50       |\n",
      "| mean 100 episode reward | 16.1     |\n",
      "| steps                   | 788      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 10       |\n",
      "| episodes                | 60       |\n",
      "| mean 100 episode reward | 15.5     |\n",
      "| steps                   | 911      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 70       |\n",
      "| mean 100 episode reward | 14.9     |\n",
      "| steps                   | 1028     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 80       |\n",
      "| mean 100 episode reward | 14.4     |\n",
      "| steps                   | 1137     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 90       |\n",
      "| mean 100 episode reward | 13.9     |\n",
      "| steps                   | 1237     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 13.5     |\n",
      "| steps                   | 1337     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 110      |\n",
      "| mean 100 episode reward | 12.7     |\n",
      "| steps                   | 1436     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 120      |\n",
      "| mean 100 episode reward | 12       |\n",
      "| steps                   | 1542     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 130      |\n",
      "| mean 100 episode reward | 12       |\n",
      "| steps                   | 1691     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 140      |\n",
      "| mean 100 episode reward | 11.2     |\n",
      "| steps                   | 1793     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 150      |\n",
      "| mean 100 episode reward | 11.3     |\n",
      "| steps                   | 1919     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 160      |\n",
      "| mean 100 episode reward | 11.4     |\n",
      "| steps                   | 2056     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 170      |\n",
      "| mean 100 episode reward | 12.2     |\n",
      "| steps                   | 2248     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 180      |\n",
      "| mean 100 episode reward | 12.6     |\n",
      "| steps                   | 2397     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 190      |\n",
      "| mean 100 episode reward | 13.7     |\n",
      "| steps                   | 2605     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 15.4     |\n",
      "| steps                   | 2878     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 210      |\n",
      "| mean 100 episode reward | 25       |\n",
      "| steps                   | 3941     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 220      |\n",
      "| mean 100 episode reward | 42.6     |\n",
      "| steps                   | 5800     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 230      |\n",
      "| mean 100 episode reward | 59.9     |\n",
      "| steps                   | 7685     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 240      |\n",
      "| mean 100 episode reward | 74.7     |\n",
      "| steps                   | 9263     |\n",
      "--------------------------------------\n",
      "Saving model to cartpole_model.pkl\n"
     ]
    }
   ],
   "source": [
    "act= train_cartpole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_cartpole(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mountaincar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_mountaincar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeprl2",
   "language": "python",
   "name": "deeprl2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
